{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Floating-point arithmetic, vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "\n",
    "**Today:** \n",
    "- Part 1: floating point, vector norms\n",
    "- Part 2: matrix norms and stability concepts\n",
    "\n",
    "**Tomorrow:** Matrix norms and unitary matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of numbers\n",
    "\n",
    "- Real numbers represent quantities: probabilities, velocities, masses, ...\n",
    "\n",
    "<img src=\"./Babylonian_numerals.png\" width=500>\n",
    "\n",
    "- It is important to know, how they are represented in the computer, which only knows about bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed point\n",
    "\n",
    "- The most straightforward format for the representation of real numbers is **fixed point** representation, also known as **Qm.n** format.\n",
    "\n",
    "- A **Qm.n** number is in the range $[-(2^m), 2^m - 2^{-n}]$, with resolution $2^{-n}$.\n",
    "\n",
    "- Total storage is $m + n + 1$ bits.\n",
    "\n",
    "- The range of numbers represented is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Floating point\n",
    "The numbers in computer memory are typically represented as **floating point numbers** \n",
    "\n",
    "A floating point number is represented as  \n",
    "\n",
    "$$\\textrm{number} = \\textrm{significand} \\times \\textrm{base}^{\\textrm{exponent}},$$\n",
    "\n",
    "where *significand* is integer, *base* is positive integer  and *exponent* is integer (can be negative), i.e.\n",
    "\n",
    "$$ 1.2 = 12 \\cdot 10^{-1}.$$\n",
    "\n",
    "- This format has a long history. It was already used in the world's first working programmable, fully automatic digital computer [Z3](https://en.wikipedia.org/wiki/Z3_(computer)) designed in 1935 and completed in 1941 in Germany by [Konrad  Zuse](https://en.wikipedia.org/wiki/Konrad_Zuse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Floating point: formula\n",
    "\n",
    "$$f = (-1)^s 2^{(p-b)} \\left( 1 + \\frac{d_1}{2} + \\frac{d_2}{2^2}  + \\ldots + \\frac{d_m}{2^m}\\right),$$\n",
    "\n",
    "where $s \\in \\{0, 1\\}$ is the sign bit, $d_i \\in \\{0, 1\\}$ is the $m$-bit mantissa, $p \\in \\mathbb{Z}; 0 \\leq p \\leq 2^e$, $e$ is the $e$-bit exponent, commonly defined as $2^e - 1$\n",
    "\n",
    "Can be thought as a uniform $m$-bit grid between two sequential powers of $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed vs Floating\n",
    "\n",
    "**Q**: What are the advantages/disadvantages of the fixed and floating points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A**:  In most cases, they work just fine.\n",
    "\n",
    "- However, fixed point represents numbers within specified range and controls **absolute** accuracy.\n",
    "\n",
    "- Floating point represent numbers with **relative** accuracy, and is suitable for the case when numbers in the computations have varying scale (i.e., $10^{-1}$ and $10^{5}$).\n",
    "\n",
    "- In practice, if speed is of no concern, use float32 or float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IEEE 754\n",
    "In modern computers, the floating point representation is controlled by [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point) which was published in **1985** and before that point different computers behaved differently with floating point numbers. \n",
    "\n",
    "IEEE 754 has:\n",
    "- Floating point representation (as described above), $(-1)^s \\times c \\times b^q$.\n",
    "- Two infinities, $+\\infty$ and $-\\infty$\n",
    "- Two zeros: +0 and -0\n",
    "- Two kinds of **NaN**: a quiet NaN (**qNaN**) and signalling NaN (**sNaN**) \n",
    "    - qNaN does not throw exception in the level of floating point unit (FPU), until you check the result of computations\n",
    "    - sNaN value throws exception from FPU if you use corresponding variable. This type of NaN can be useful for initialization purposes\n",
    "    - C++11 proposes [standard interface](https://en.cppreference.com/w/cpp/numeric/math/nan) for creating different NaNs \n",
    "- Rules for **rounding**\n",
    "- Rules for $\\frac{0}{0}, \\frac{1}{-0}, \\ldots$\n",
    "\n",
    "Possible values are defined with\n",
    "- base $b$\n",
    "- accuracy $p$ - number of digits\n",
    "- maximum possible value $e_{\\max}$\n",
    "\n",
    "and have the following restrictions \n",
    "- $ 0 \\leq c \\leq b^p - 1$\n",
    "- $1 - e_{\\max} \\leq q + p - 1 \\leq e_{\\max}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The two most common format, single & double\n",
    "\n",
    "The two most common formats, called **binary32** and **binary64** (called also **single** and **double** formats). Recently, the format **binary16** plays important role in learning deep neural networks.\n",
    "\n",
    "| Name | Common Name | Base | Digits | Emin | Emax |\n",
    "|------|----------|----------|-------|------|------|\n",
    "|binary16| half precision | 2 | 11 | -14 | + 15 |\n",
    "|binary32| single precision | 2 | 24 | -126 | + 127 |  \n",
    "|binary64| double precision | 2 | 53 | -1022 | +1023 |  \n",
    "\n",
    "<img src=\"./double64.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "- For a number +0\n",
    "    - *sign* is 0\n",
    "    - *exponent* is 00000000000\n",
    "    - *fraction* is all zeros\n",
    "- For a number -0\n",
    "    - *sign* is 1\n",
    "    - *exponent* is 00000000000\n",
    "    - *fraction* is all zeros\n",
    "- For +infinity\n",
    "    - *sign* is 0\n",
    "    - *exponent* is 11111111111\n",
    "    - *fraction* is all zeros\n",
    "\n",
    "**Q**: what about -infinity and NaN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy and memory\n",
    "\n",
    "The **relative accuracy** of single precision is $10^{-7}-10^{-8}$, while for double precision is $10^{-14}-10^{-16}$.\n",
    "\n",
    "<font color='red'> Crucial note 1: </font> A **float16** takes **2 bytes**, **float32** takes **4 bytes**, **float64**, or double precision, takes **8 bytes.**\n",
    "\n",
    "<font color='red'> Crucial note 2: </font> These are the only two floating point-types supported in hardware (float32 and float64) + GPU/TPU different float types are supported.\n",
    "\n",
    "<font color='red'> Crucial note 3: </font> You should use **double precision** in computational science and engineering and **float** on GPU/Data Science.\n",
    "\n",
    "\n",
    "Also, half precision can be useful in training deep neural network, see this [paper](https://arxiv.org/pdf/1905.12334.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does number representation format affect training of neural networks (NN)?\n",
    "\n",
    "- Weights in layers (fully-connected, convolutional, activation functions) can be stored with different accuracies\n",
    "- It is important to improve energy efficiency of the devices that are used to train NNs\n",
    "- Project [DeepFloat](https://github.com/facebookresearch/deepfloat) from Facebook demonstrates how re-develop floating point operations in a way to ensure efficiency in training NNs, more details see in this [paper](https://arxiv.org/pdf/1811.01721.pdf)\n",
    "- Affect of the real numbers representation on the gradients of activation functions\n",
    "- Typically, the first digit is one.\n",
    "- Subnormal numbers have first digit 0 to represent zeros and numbers close to zero.\n",
    "- Subnormal numbers fill the gap between positive and negative\n",
    "- They have performance issues, often flushed to zero by default.\n",
    "\n",
    "<img width=500, src=\"./grad_norm_fp16.png\">\n",
    "\n",
    "- And on the learning curves\n",
    "\n",
    "<img width=500, src=\"./train_val_curves.png\">\n",
    "\n",
    "Plots are taken from [this paper](https://arxiv.org/pdf/1710.03740.pdf%EF%BC%89%E3%80%82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## bfloat16 (Brain Floating Point)\n",
    "\n",
    "- This format occupies 16 bits\n",
    "    - 1 bit for sign\n",
    "    - 8 bits for exponent\n",
    "    - 7 bits for fraction\n",
    "    <img src=\"./bfloat16.png\">\n",
    "- Truncated single precision format from IEEE standard\n",
    "- What is the difference between float32 and float16?\n",
    "- This format is utilized in Intel FPGA, Google TPU, Xeon CPUs and other platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor Float from Nvidia ([blog post about this format](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/))\n",
    "\n",
    "- Comaprison with other formats\n",
    "\n",
    "<img src=\"./tensor_float_cf.png\">\n",
    "\n",
    "- Results\n",
    "\n",
    "<img src=\"./TF32-BERT.png\">\n",
    "\n",
    "- PyTorch and Tensorflow supported this format are available in [Nvidia NCG](https://ngc.nvidia.com/catalog/all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixed precision ([docs from Nvidia](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html))\n",
    "\n",
    "- Main idea:\n",
    "    - Maintain copy of weights in single precision\n",
    "    - Then in every iteration\n",
    "        - Make a copy of weights in half-precision\n",
    "        - Forward pass with weights in half-precision\n",
    "        - Multiply the loss by the scaling factor $S$\n",
    "        - Backward pass again in half precision\n",
    "        - Multiply the weight gradient with $1/S$\n",
    "        - Complete the weight update (including gradient clipping, etc.)\n",
    "    - Scaling factor $S$ is a hyper-parameter\n",
    "    - Constant: a value so that its product with the maximum absolute gradient value is below 65504 (the maximum value representable in half precision).\n",
    "    - Dynamic update based on the current gradient statistics\n",
    "- Performance comparison\n",
    "<img src=\"./mixed_precision_res.png\" width=500>\n",
    "\n",
    "- Automatic mixed-precision extensions exist to simplify turning this option on, more details [here](https://developer.nvidia.com/automatic-mixed-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative to the IEEE 754 standard\n",
    "\n",
    "Issues in IEEE 754:\n",
    "- overflow to infinity or zero\n",
    "- many different NaNs\n",
    "- invisible rounding errors\n",
    "- accuracy is very high or very poor\n",
    "- subnormal numbers – numbers between 0 and minimal possible represented number, i.e. significand starts from zero\n",
    "\n",
    "Concept of **posits** can replace floating point numbers, see [this paper](http://www.johngustafson.net/pdfs/BeatingFloatingPoint.pdf)\n",
    "\n",
    "<img width=600 src=\"./posit.png\">\n",
    "\n",
    "- represent numbers with some accuracy, but provide limits of changing\n",
    "- no overflows!\n",
    "- example of a number representation \n",
    "\n",
    "<img width=600 src=\"./posit_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Division accuracy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259246\n",
      "0.1040364727377892\n",
      "6.437311e-08\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "#c = random.random()\n",
    "#print(c)\n",
    "c = jnp.float32(0.925924589693)\n",
    "print(c)\n",
    "a = jnp.float32(8.9)\n",
    "b = jnp.float32(c / a)\n",
    "print('{0:10.16f}'.format(b))\n",
    "print(abs(a * b - c)/abs(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Square root accuracy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = jnp.float32(1.585858)\n",
    "b = jnp.sqrt(a)\n",
    "print(b.dtype)\n",
    "print('{0:10.64f}'.format(abs(b * b - a)/abs(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponent accuracy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = jnp.float32(50.081818)\n",
    "b = jnp.exp(a)\n",
    "print(b.dtype)\n",
    "print(jnp.log(b) - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of demos\n",
    "\n",
    "- For some values the inverse functions give exact answers\n",
    "- The relative accuracy should be preserved due to the IEEE standard\n",
    "- Does not hold for many modern GPU\n",
    "- More details about adoptation of IEEE 754 standard for GPU you can find [here](https://docs.nvidia.com/cuda/floating-point/index.html#considerations-for-heterogeneous-world) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss of significance\n",
    "\n",
    "- Many operations lead to the loss of digits [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance)\n",
    "- For example, it is a bad idea to subtract two big numbers that are close, the difference will have fewer correct digits\n",
    "- This is related to algorithms and their properties (forward/backward stability), which we will discuss later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summation algorithm\n",
    "\n",
    "However, the rounding errors can depend on the algorithm.\n",
    "\n",
    "- Consider the simplest problem: given $n$ floating point numbers $x_1, \\ldots, x_n$  \n",
    "\n",
    "- Compute their sum\n",
    "\n",
    "$$S = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.$$\n",
    "\n",
    "- The simplest algorithm is to add one-by-one \n",
    "\n",
    "- What is the actual error for such algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Na&iuml;ve algorithm\n",
    "\n",
    "Na&iuml;ve algorithm adds numbers one-by-one: \n",
    "\n",
    "$$y_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.$$\n",
    "\n",
    "- The **worst-case** error is then proportional to $\\mathcal{O}(n)$, while **mean-squared** error is $\\mathcal{O}(\\sqrt{n})$.\n",
    "\n",
    "- The **Kahan algorithm** gives the worst-case error bound $\\mathcal{O}(1)$ (i.e., independent of $n$).  \n",
    "\n",
    "- <font color='red'> Can you find the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kahan summation\n",
    "The following algorithm gives $2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2)$ error, where $\\varepsilon$ is the machine precision.\n",
    "\n",
    "- The reason of the loss of significance in summation is operating with numbers of different magnitude\n",
    "- The main idea of Kahan summation is to keep track of small errors and aggregate them in separate variable\n",
    "- This approach is called *compensated summation*\n",
    "\n",
    "```python\n",
    "s = 0\n",
    "c = 0\n",
    "for i in range(len(x)):\n",
    "    y = x[i] - c\n",
    "    t = s + y\n",
    "    c = (t - s) - y\n",
    "    s = t\n",
    "```\n",
    "\n",
    "- There exists more advanced tricks to process this simple operation that are used for example in ```fsum``` function from ```math``` package, implementation check out [here](https://github.com/python/cpython/blob/d267006f18592165ed97e0a9c2494d3bce25fc2b/Modules/mathmodule.c#L1087)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in np sum: -8.3e-07\n",
      "Error in Kahan sum Numba: 4.7e-08\n",
      "Error in Kahan sum JAX: 0.0e+00\n",
      "Error in dumb sum: -1.0e-03\n",
      "Error in math fsum: 1.3e-11\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from numba import jit as numba_jit\n",
    "\n",
    "n = 10 ** 7\n",
    "sm = 1e-10\n",
    "x = jnp.ones(n, dtype=jnp.float32) * sm\n",
    "x = x.at[0].set(1)\n",
    "#x = jax.ops.index_update(x, [0], 1.)\n",
    "true_sum = 1.0 + (n - 1)*sm\n",
    "approx_sum = jnp.sum(x)\n",
    "math_fsum = math.fsum(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def dumb_sum(x):\n",
    "    s = jnp.float32(0.0)\n",
    "    def b_fun(i, val):\n",
    "        return val + x[i] \n",
    "    s = jax.lax.fori_loop(0, len(x), b_fun, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "@numba_jit(nopython=True)\n",
    "def kahan_sum_numba(x):\n",
    "    s = np.float32(0.0)\n",
    "    c = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "    return s\n",
    "\n",
    "@jax.jit\n",
    "def kahan_sum_jax(x):\n",
    "    s = jnp.float32(0.0)\n",
    "    c = jnp.float32(0.0)\n",
    "    def b_fun2(i, val):\n",
    "        s, c = val\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "        return s, c\n",
    "    s, c = jax.lax.fori_loop(0, len(x), b_fun2, (s, c))\n",
    "    return s\n",
    "\n",
    "k_sum_numba = kahan_sum_numba(np.array(x))\n",
    "k_sum_jax = kahan_sum_jax(x)\n",
    "d_sum = dumb_sum(x)\n",
    "print('Error in np sum: {0:3.1e}'.format(approx_sum - true_sum))\n",
    "print('Error in Kahan sum Numba: {0:3.1e}'.format(k_sum_numba - true_sum))\n",
    "print('Error in Kahan sum JAX: {0:3.1e}'.format(k_sum_jax - true_sum))\n",
    "print('Error in dumb sum: {0:3.1e}'.format(d_sum - true_sum))\n",
    "print('Error in math fsum: {0:3.1e}'.format(math_fsum - true_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "test_list = [1, 1e20, 1, -1e20]\n",
    "print(math.fsum(test_list))\n",
    "print(jnp.sum(jnp.array(test_list)))\n",
    "print(1 + 1e20 + 1 - 1e20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of floating-point \n",
    "- You should be really careful with floating point numbers, since it may give you incorrect answers due to rounding-off errors.\n",
    "\n",
    "- For many standard algorithms, the stability is well-understood and problems can be easily detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "- In NLA we typically work not with **numbers**, but with **vectors**\n",
    "- Recall that a vector in a fixed basis of size $n$ can be represented as a 1D array with $n$ numbers \n",
    "- Typically, it is considered as an $n \\times 1$ matrix (**column vector**)\n",
    "\n",
    "**Example:** \n",
    "Polynomials with degree $\\leq n$ form a linear space. \n",
    "Polynomial $ x^3 - 2x^2 + 1$ can be considered as a vector $\\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix}$ in the basis $\\{x^3, x^2, x, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector norm\n",
    "\n",
    "- Vectors typically provide an (approximate) description of a physical (or some other) object \n",
    "\n",
    "- One of the main question is **how accurate** the approximation is (1%, 10%)\n",
    "\n",
    "- What is an acceptable representation, of course, depends on the particular applications. For example:\n",
    "    - In partial differential equations accuracies $10^{-5} - 10^{-10}$ are the typical case\n",
    "    - In data-based applications sometimes an error of $80\\%$ is ok, since the interesting signal is corrupted by a huge noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances and norms\n",
    "\n",
    "- Norm is a **qualitative measure of smallness of a vector** and is typically denoted as $\\Vert x \\Vert$.\n",
    "\n",
    "The norm should satisfy certain properties:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (triangle inequality)\n",
    "- If $\\Vert x \\Vert = 0$ then $x = 0$\n",
    "\n",
    "The distance between two vectors is then defined as\n",
    "\n",
    "$$ d(x, y) = \\Vert x - y \\Vert. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard norms\n",
    "The most well-known and widely used norm is **euclidean norm**:\n",
    "\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "\n",
    "which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $p$-norm\n",
    "Euclidean norm, or $2$-norm, is a subclass of an important class of $p$-norms:\n",
    "\n",
    "$$ \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}. $$\n",
    "\n",
    "There are two very important special cases:\n",
    "- Infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n",
    "\n",
    "$$ \\Vert x \\Vert_{\\infty} = \\max_i | x_i| $$\n",
    "\n",
    "<img src=\"chebyshev.jpeg\">\n",
    "\n",
    "- $L_1$ norm (or **Manhattan distance**) which is defined as the sum of modules of the elements of $x$: \n",
    "\n",
    "$$ \\Vert x \\Vert_1 = \\sum_i |x_i| $$\n",
    "  \n",
    "<img src=\"manhattan.jpeg\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will give examples where $L_1$ norm is very important: it all relates to the **compressed sensing** methods \n",
    "that emerged in the mid-00s as one of the most popular research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equivalence of the norms\n",
    "All norms are equivalent in the sense that\n",
    "\n",
    "$$ C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_* $$  \n",
    "\n",
    "for some positive constants $C_1(n), C_2(n)$, $x \\in \\mathbb{R}^n$ for any pairs of norms $\\Vert \\cdot \\Vert_*$ and $\\Vert \\cdot \\Vert_{**}$. The equivalence of the norms basically means that if the vector is small in one norm, it is small in another norm. However, the constants can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing norms in Python\n",
    "\n",
    "The NumPy package has all you need for computing norms: ```np.linalg.norm``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "a = jnp.ones(n)\n",
    "b = a + 1e-3 * jax.random.normal(jax.random.PRNGKey(0), (n,))\n",
    "print('Relative error in L1 norm:', jnp.linalg.norm(a - b, 1) / jnp.linalg.norm(b, 1))\n",
    "print('Relative error in L2 norm:', jnp.linalg.norm(a - b) / jnp.linalg.norm(b))\n",
    "print('Relative error in Chebyshev norm:', jnp.linalg.norm(a - b, jnp.inf) / jnp.linalg.norm(b, jnp.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unit disks in different norms\n",
    "\n",
    "- A unit disk is a set of point such that $\\Vert x \\Vert \\leq 1$\n",
    "- For the euclidean norm a unit disk is a usual disk\n",
    "- For other norms unit disks look very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "p = 0.5 # Which norm do we use\n",
    "M = 4000 # Number of sampling points\n",
    "b = []\n",
    "for i in range(M):\n",
    "    a = jax.random.normal(jax.random.PRNGKey(i), (1, 2))\n",
    "    if jnp.linalg.norm(a[i, :], p) <= 1:\n",
    "        b.append(a[i, :])\n",
    "b = jnp.array(b)\n",
    "plt.plot(b[:, 0], b[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why $L_1$-norm can be important?\n",
    "\n",
    "$L_1$ norm, as it was discovered quite recently, plays an important role in **compressed sensing**. \n",
    "\n",
    "The simplest formulation of the considered problem is as follows:\n",
    "\n",
    "- You have some observations $f$ \n",
    "- You have a linear model $Ax = f$, where $A$ is an $n \\times m$ matrix, $A$ is **known**\n",
    "- The number of equations, $n$, is less than the number of unknowns, $m$\n",
    "\n",
    "The question: can we find the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution is obviously non-unique, so a natural approach is to find the solution that is minimal in the certain sense:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\Vert x \\Vert \\rightarrow \\min_x \\\\\n",
    "\\mbox{subject to } & Ax = f\n",
    "\\end{align*}\n",
    "\n",
    "- Typical choice of $\\Vert x \\Vert = \\Vert x \\Vert_2$ leads to the **linear least squares problem** (and has been used for ages).  \n",
    "\n",
    "- The choice $\\Vert x \\Vert = \\Vert x \\Vert_1$ leads to the [**compressed sensing**](https://en.wikipedia.org/wiki/Compressed_sensing)\n",
    "- It typically yields the **sparsest solution**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a stable algorithm?\n",
    "\n",
    "And we finalize the lecture by the concept of **stability**.\n",
    "\n",
    "- Let $x$ be an object (for example, a vector) \n",
    "- Let $f(x)$ be the function (functional) you want to evaluate \n",
    "\n",
    "You also have a **numerical algorithm** ``alg(x)`` that actually computes **approximation** to $f(x)$.  \n",
    "\n",
    "The algorithm is called **forward stable**, if $$\\Vert \\text{alg}(x) - f(x) \\Vert  \\leq \\varepsilon $$  \n",
    "\n",
    "The algorithm is called **backward stable**, if for any $x$ there is a close vector $x + \\delta x$ such that\n",
    "\n",
    "$$\\text{alg}(x) = f(x + \\delta x)$$\n",
    "\n",
    "and $\\Vert \\delta x \\Vert$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical example\n",
    "A classical example is the **solution of linear systems of equations** using Gaussian elimination which is similar to LU factorization (more details later)\n",
    "\n",
    "We consider the **Hilbert matrix** with the elements\n",
    "\n",
    "$$A = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.$$\n",
    "\n",
    "And consider a linear system\n",
    "\n",
    "$$Ax = f.$$\n",
    "\n",
    "We will look into matrices in more details in the next lecture, and for linear systems in the upcoming weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] # Hilbert matrix\n",
    "A = jnp.array(a)\n",
    "#rhs =  jax.random.normal(jax.random.PRNGKey(0), (n,))\n",
    "rhs = jnp.ones(n)\n",
    "sol = jnp.linalg.solve(A, rhs)\n",
    "print(jnp.linalg.norm(A @ sol - rhs)/jnp.linalg.norm(rhs))\n",
    "plt.plot(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rhs = jnp.ones(n)\n",
    "sol = jnp.linalg.solve(A, rhs)\n",
    "print(jnp.linalg.norm(A @ sol - rhs)/jnp.linalg.norm(rhs))\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More examples of instability\n",
    "\n",
    "How to compute the following functions in numerically stable manner?\n",
    "\n",
    "- $\\log(1 - \\tanh^2(x))$\n",
    "- $\\text{SoftMax}(x)_j = \\dfrac{e^{x_j}}{\\sum\\limits_{i=1}^n e^{x_i}}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "u = 300\n",
    "eps = 1e-6\n",
    "print(\"Original function:\", jnp.log(1 - jnp.tanh(u)**2))\n",
    "eps_add = jnp.log(1 - jnp.tanh(u)**2 + eps)\n",
    "print(\"Attempt to improve stability by adding a small constant:\", eps_add)\n",
    "print(\"Use more numerically stable form:\", jnp.log(4) - 2 * jnp.log(jnp.exp(-u) + jnp.exp(u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n",
    "x = jax.ops.index_update(x, [0], 1000)\n",
    "print(jnp.exp(x) / jnp.sum(jnp.exp(x)))\n",
    "print(jnp.exp(x - jnp.max(x)) / jnp.sum(jnp.exp(x - jnp.max(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Floating point  (double, single, number of bytes), rounding error\n",
    "- Norms are measures of smallness, used to compute the accuracy\n",
    "- $1$, $p$ and Euclidean norms \n",
    "- $L_1$ is used in compressed sensing as a surrogate for sparsity (later lectures) \n",
    "- Forward/backward error (and stability of algorithms)  (later lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "\n",
    "- Matrix norms: what is the difference between matrix and vector norms\n",
    "- Unitary matrices, including elementary unitary matrices."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
